* Multi-Objective Hyper-Parameter Optimization for Quality Estimation of Machine Translation

[[Jakob_Hauser_Thesis.pdf][Full PDF here]]
** Abstract
Hyper-parameter optimization (HPO) is an essential part of most Machine Learning (ML) pipelines. Currently, the predominant usage of HPO for ML falls in the category of Single Objective Optimization (SOO).  While the SOO approach of HPO usually benefits from lower runtime and better convergence, it restricts the tuning to only one requirement. In most cases, such an optimization fails to reflect the real-world criteria of the implementation. It is surprising that SOO is still the predominant approach for tuning ML models, while in many other disciplines (e.g. Aerospace Engineering) optimizing for only one of the requirements would be unacceptable. Usually, when designing a product or a process, the goal is to find an optimum across many conflicting objectives. Also in real-world software deployments, Deep Learning models are supposed to strike a balance between raw predictive performance and computational complexity. To achieve this the hyperparameters of Deep Learning models have to be optimized. Therefore, we choose to explore HPO using Multi-Objective Optimization (MOO) in terms of the aforementioned criteria. We evaluate 4 different (both multi and single-objective) optimization algorithms, and investigate their performance in regards to optimizing a Quality Estimation of Machine Translation model. Our chosen optimization criteria are predictive performance and floating-point operations for model inference (used as a proxy measure for computational complexity). While MOO shows some promise the findings of our investigation are inconclusive.

* Usage / Replication
** Requirements
The requirements can be installed using poetry (recommended) or using the requirements.txt file.
Additionally pytorch has to be installed manually according to cpu or gpu usage and cuda version.

** Adding additional data or changing the data used
Place a folder containing the data in the =data/= folder (mirroring the structure of the already existing files) and run the =concat_data.py= script from inside the =utils= folder.
Then run the following line of bash from within the =utils= folder.
#+begin_src bash
cat ../data/combined_data/train.src ../data/combined_data/train.mt > ../data/combined_data/train.src-trg
#+end_src

Then execute the =train_bpe_model.py= script from within the =utils= folder.

** Running the Experiment script
Change the respective variables in the =main()= function according to your requirements.
run =python experiment.py= from inside the root of the repository

** Evaluation
The code for the evaluation can be found in the =MOO_EVAL= folder. Github should render the markdown of the literate programming =evaluation.org= file properly when one clicks on it. Otherwise all the code is available in the =evaluation.py= file.
